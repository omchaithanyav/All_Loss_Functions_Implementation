{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "Killed\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error (MSE) Loss\n",
    "# Binary Cross-Entropy Loss\n",
    "# Weighted Binary Cross-Entropy Loss\n",
    "# Categorical Cross-Entropy Loss\n",
    "# Sparse Categorical Cross-Entropy Loss\n",
    "# Dice Loss\n",
    "# KL Divergence Loss\n",
    "# Mean Absolute Error (MAE) / L1 Loss\n",
    "# Huber Loss\n",
    "\n",
    "# https://arjun-sarkar786.medium.com/implementation-of-all-loss-functions-deep-learning-in-numpy-tensorflow-and-pytorch-e20e72626ebd\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error (MSE) Loss\n",
    "\"\"\"\n",
    "Mean Squared Error (MSE) loss is a commonly used loss function in regression problems, where the goal is to predict a continuous variable.\n",
    "The loss is calculated as the average of the squared differences between the predicted and true values. \n",
    "\n",
    "The formula for MSE loss is:\n",
    "MSE loss = (1/n) * sum((y_pred — y_true)²)\n",
    "\n",
    "Where:\n",
    "\n",
    "n is the number of samples in the dataset\n",
    "y_pred is the predicted value of the target variable\n",
    "y_true is the true value of the target variable\n",
    "\n",
    "The MSE loss is sensitive to outliers and can penalize large errors heavily, \n",
    "which may not be desirable in some cases. In such cases, other loss functions like Mean Absolute Error (MAE) or Huber Loss may be used instead.\n",
    "\"\"\"\n",
    "\n",
    "# Using Numpy\n",
    "def mse_loss_np(y_pred, y_true):\n",
    "    n = len(y_true) # n is numer of samples\n",
    "    mse = np.sum((y_pred - y_true) ** 2) / n # y_pred and y_true are NumPy arrays\n",
    "    return mse\n",
    "\n",
    "# Using Tensorflow\n",
    "def mse_loss_tf(y_pred, y_true):\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    mse_loss = mse(y_true, y_pred)\n",
    "    return mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross-Entropy Loss\n",
    "\"\"\"\n",
    " Binary Cross-Entropy loss, also known as log loss, is a common loss function used in binary classification problems. \n",
    "It measures the difference between the predicted probability distribution and the actual binary label distribution.\n",
    "\n",
    "The formula for binary cross-entropy loss is as follows:\n",
    "\n",
    "L(y, ŷ) = -[y * log(ŷ) + (1 — y) * log(1 — ŷ)]\n",
    "\n",
    "where y is the true binary label (0 or 1), ŷ is the predicted probability (ranging from 0 to 1), and log is the natural logarithm.\n",
    "\n",
    "The first term of the equation calculates the loss when the true label is 1, and the second term calculates the loss when the true label is 0. \n",
    "The overall loss is the sum of both terms.\n",
    "\n",
    "When the predicted probability is close to the true label, the loss is low, and when the predicted probability is far from the true label, \n",
    "the loss is high. This loss function is commonly used in neural network models that use sigmoid activation functions in the output layer \n",
    "to predict binary labels.\n",
    "\"\"\"\n",
    "\n",
    "# y_true - true labels\n",
    "# y_pred - predicted probabilities\n",
    "\n",
    "# using Numpy\n",
    "def log_loss_np(y_pred, y_true):\n",
    "    # calculate the binary cross-entropy loss\n",
    "    loss = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)).mean()\n",
    "    return loss\n",
    "\n",
    "# using Tensorflow\n",
    "def log_loss_tf(y_pred, y_true):\n",
    "    # define the loss function\n",
    "    bce_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    # calculate the loss\n",
    "    loss = bce_loss(y_true, y_pred)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Cross-Entropy Loss\n",
    "\"\"\"\n",
    "The categorical cross-entropy loss is a popular loss function used in multi-class classification problems. \n",
    "It measures the dissimilarity between the true labels and the predicted probabilities for each class.\n",
    "\n",
    "The formula for categorical cross-entropy loss is:\n",
    "\n",
    "L = -1/N * sum(sum(Y * log(Y_hat)))\n",
    "\n",
    "where Y is a matrix of true labels in one-hot encoding format, Y_hat is a matrix of predicted probabilities for each class, \n",
    "N is the number of samples, and log represents the natural logarithm.\n",
    "\n",
    "In this formula, Y has a shape of (N, C), where N is the number of samples and C is the number of classes. \n",
    "Each row of Y represents the true label distribution for a single sample, with a value of 1 in the column \n",
    "corresponding to the true label and 0 in all other columns.\n",
    "\n",
    "Similarly, Y_hat has a shape of (N, C), where each row represents the predicted probability distribution for a single sample, \n",
    "with a probability value for each class.\n",
    "\n",
    "The log function is applied element-wise to the predicted probability matrix Y_hat. \n",
    "The sum function is used twice to sum over both dimensions of the Y matrix.\n",
    "\n",
    "The resulting value L represents the average cross-entropy loss over all N samples in the dataset. \n",
    "The goal of training a neural network is to minimize this loss function.\n",
    "\n",
    "The loss function penalizes the model more heavily for making large errors in predicting classes with low probabilities. \n",
    "The goal is to minimize the loss function, which means making the predicted probabilities as close to the true labels as possible.\n",
    "\"\"\"\n",
    "\n",
    "# using Numpy\n",
    "def categorical_crossentropy_np():\n",
    "    # define true labels and predicted probabilities as NumPy arrays\n",
    "    # y_true = np.array([[0, 1, 0], [0, 0, 1], [1, 0, 0]])\n",
    "    # y_pred = np.array([[0.8, 0.1, 0.1], [0.2, 0.3, 0.5], [0.1, 0.6, 0.3]])\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = -1/len(y_true) * np.sum(np.sum(y_true * np.log(y_pred)))\n",
    "    return loss\n",
    "\n",
    "# using Tensorflow\n",
    "def categorical_crossentropy_tf():\n",
    "    # create the loss object\n",
    "    cce_loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    # calculate the loss\n",
    "    loss = cce_loss(y_true, y_pred)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse Categorical Cross-Entropy Loss\n",
    "\"\"\"\n",
    "The sparse categorical cross-entropy loss is similar to the categorical cross-entropy loss, \n",
    "but it is used when the true labels are provided as integers rather than one-hot encoding. \n",
    "It is commonly used as a loss function in multi-class classification problems.\n",
    "\n",
    "The formula for sparse categorical cross-entropy loss is:\n",
    "\n",
    "L = -1/N * sum(log(Y_hat_i))\n",
    "\n",
    "where Y_hat_i is the predicted probability for the true class label i for each sample, and N is the number of samples.\n",
    "\n",
    "In other words, the formula calculates the negative logarithm of the predicted probability for the true class label for each sample, \n",
    "and then averages these values over all samples.\n",
    "\n",
    "Unlike the categorical cross-entropy loss, which uses a one-hot encoding for the true labels, the sparse categorical cross-entropy loss \n",
    "uses integer labels directly. The true label for each sample is represented as a single integer value i between 0 and C-1, \n",
    "where C is the number of classes.\n",
    "\"\"\"\n",
    "\n",
    "# using Numpy\n",
    "def sparse_categorical_crossentropy_np(y_true, y_pred):\n",
    "    # convert true labels to one-hot encoding\n",
    "    y_true_onehot = np.zeros_like(y_pred)\n",
    "    y_true_onehot[np.arange(len(y_true)), y_true] = 1\n",
    "    # calculate loss\n",
    "    loss = -np.mean(np.sum(y_true_onehot * np.log(y_pred), axis=-1))\n",
    "    return loss\n",
    "\n",
    "# using Tensorflow\n",
    "def sparse_categorical_crossentropy_np(y_true, y_pred):\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=False)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error (MAE) Loss / L1 Loss\n",
    "\"\"\"\n",
    "L1 loss, also known as mean absolute error (MAE) loss, is a common loss function used in deep learning for regression tasks. \n",
    "It measures the absolute differences between the predicted and true values of the target variable.\n",
    "\n",
    "The formula for L1 loss is:\n",
    "\n",
    "L1 loss = 1/n * Σ|y_pred — y_true|\n",
    "\n",
    "where n is the number of samples, y_pred is the predicted value, and y_true is the true value.\n",
    "\n",
    "In simpler terms, L1 loss is the average of the absolute differences between the predicted and true values. \n",
    "It is less sensitive to outliers than the mean squared error (MSE) loss, making it a good choice for models that can be affected by outliers.\n",
    "\"\"\"\n",
    "\n",
    "# using Numpy\n",
    "def l1_loss_np(y_pred, y_true):\n",
    "    loss = np.mean(np.abs(y_pred - y_true))\n",
    "    return loss\n",
    "\n",
    "# using Tensorflow\n",
    "def l1_loss_tf(y_pred, y_true):\n",
    "    loss = tf.reduce_mean(tf.abs(y_pred - y_true))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huber Loss\n",
    "\"\"\"\n",
    "Huber loss is a loss function used in regression tasks that is less sensitive to outliers than Mean Squared Error (MSE) loss. \n",
    "It is defined as a combination of the MSE loss and Mean Absolute Error (MAE) loss, \n",
    "where the loss function is MSE for small errors and MAE for larger errors. This makes Huber loss more robust to outliers than MSE loss.\n",
    "\"\"\"\n",
    "\n",
    "# using Numpy\n",
    "def huber_loss(y_pred, y_true, delta=1.0):\n",
    "    error = y_pred - y_true\n",
    "    abs_error = np.abs(error)\n",
    "    quadratic = np.minimum(abs_error, delta)\n",
    "    linear = (abs_error - quadratic)\n",
    "    return np.mean(0.5 * quadratic ** 2 + delta * linear)\n",
    "\n",
    "# using Tensorflow\n",
    "def huber_loss(y_pred, y_true, delta=1.0):\n",
    "    error = y_pred - y_true\n",
    "    abs_error = tf.abs(error)\n",
    "    quadratic = tf.minimum(abs_error, delta)\n",
    "    linear = (abs_error - quadratic)\n",
    "    return tf.reduce_mean(0.5 * quadratic ** 2 + delta * linear)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
